{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import quote\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a user agent string\n",
    "user_agent = 'MandarineCorp (clementine.naim@epfl.ch)'\n",
    "\n",
    "# Specify the headers with the user agent\n",
    "headers = {\n",
    "    'User-Agent': user_agent,\n",
    "    'accept': 'application/json'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET THE PHILOSOPHIES FROM THE PAGE : List of philosophies\n",
    "\n",
    "# Specify the API endpoint URL\n",
    "api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# Specify the parameters for the API request\n",
    "params = {\n",
    "    'action': 'query',\n",
    "    'prop': 'revisions',\n",
    "    'titles': 'List_of_philosophies',\n",
    "    'rvslots': '*',\n",
    "    'rvprop': 'content',\n",
    "    'formatversion': 2,\n",
    "    'format': 'json'\n",
    "}\n",
    "\n",
    "# API request\n",
    "response = requests.get(api_url, params=params)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the content of the first revision\n",
    "    revisions = data['query']['pages'][0]['revisions']\n",
    "    \n",
    "    if revisions:\n",
    "        content = revisions[0]['slots']['main']['content']\n",
    "        \n",
    "        # Use a regular expression to find section titles\n",
    "        section_titles = re.findall(r'\\[\\[([^|\\]]+)(?:\\|[^]]+)?\\]\\]', content)\n",
    "        \n",
    "        while section_titles[0][0]!='A' or section_titles[-1][0]!='Z':\n",
    "            if section_titles[0][0]!='A':\n",
    "                section_titles.pop(0)\n",
    "            if section_titles[-1][0]!='Z':\n",
    "                section_titles.pop(-1)\n",
    "\n",
    "        #print(section_titles)\n",
    "    else:\n",
    "        print(\"No content found.\")\n",
    "else:\n",
    "    # Print an error message if the request was unsuccessful\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the views for the diffrent languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_languages = ['fr', 'ja', 'de', 'it','da','nl','no','sr','sv','ko','fi'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_list(languages_list, page_titles):\n",
    "    api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    language_links = []\n",
    "\n",
    "    for page in page_titles : \n",
    "\n",
    "    # Make separate requests for each language\n",
    "        for lang in target_languages:\n",
    "            params = {\n",
    "                'action': 'query',\n",
    "                'titles': page,\n",
    "                'prop': 'langlinks',\n",
    "                'llprop': 'url',\n",
    "                'format': 'json',\n",
    "                'lllang': lang,\n",
    "            }\n",
    "\n",
    "            # Make the API request\n",
    "            response = requests.get(api_url, params=params)\n",
    "            data = response.json()\n",
    "\n",
    "            # Check if the request was successful (status code 200)\n",
    "            if response.status_code == 200:\n",
    "                # Extract language links from the API response\n",
    "                pages = data['query']['pages']\n",
    "                page_id = next(iter(pages))\n",
    "                langlinks = pages[page_id].get('langlinks', [])\n",
    "                for link in langlinks:\n",
    "                    language_links.append( link['url'])\n",
    "            else:\n",
    "                print(f\"Error for language {lang}: {response.status_code}\")\n",
    "    return language_links\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_country_code_and_title(link): #finds the country code from a given link, used in  get_page_views_by_languages(links)\n",
    "    state=0\n",
    "    code=\"\"\n",
    "    title=\"\"\n",
    "    for i in range(len(link)):\n",
    "        if state==2 and link[i]==\".\":\n",
    "            state+=1\n",
    "        if state==2:\n",
    "            code+=link[i]\n",
    "        if state==5:\n",
    "            title+=link[i]\n",
    "        if link[i]==\"/\":\n",
    "            state+=1\n",
    "    return code,title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_views_by_languages(links,page_titles):\n",
    "    philo_views=pd.DataFrame()\n",
    "    i=0\n",
    "    index_subject=0\n",
    "    for link in links:\n",
    "        # Define a user agent to have acces to the API \n",
    "        user_agent = 'MandarineCorp (clementine.naim@epfl.ch)'\n",
    "        # Specify the headers with the user agent\n",
    "        headers = {\n",
    "            'User-Agent': user_agent,\n",
    "            'accept': 'application/json'\n",
    "        \n",
    "        }\n",
    "        # Find country code:\n",
    "        code, title = find_country_code_and_title(link)\n",
    "        \n",
    "        subject=page_titles[index_subject]\n",
    "        url = f'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{code}.wikipedia.org/all-access/all-agents/{title}/monthly/2018010100/2023010100'\n",
    "        # Making a GET request\n",
    "        response_views = requests.get(url,headers=headers)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response_views.status_code == 200:\n",
    "            # Print the response content\n",
    "            data = response_views.json()\n",
    "        else:\n",
    "            # Print an error message if the request was unsuccessful\n",
    "            print(f\"Error: {response_views.status_code}\")\n",
    "            print(title)\n",
    "\n",
    "\n",
    "        # Extract the 'items' list from the data\n",
    "        items_list = data['items']\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(items_list)\n",
    "        df[\"subject\"]=subject\n",
    "        df[\"code\"] =code\n",
    "        philo_views = pd.concat([philo_views,df])\n",
    "        i+=1\n",
    "        if i==11 :\n",
    "            index_subject+=1\n",
    "            i=0\n",
    "    return philo_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
